# Non-invasive Brain-computer-interface for Remote Rehab and Training of Motor Skills

**Author**: *Martin Geiger* 

**Supervisor**: *Dr. Katja Kornysheva*

**Year**: *2024*

## Project Description
Current approaches to treat Parkinson’s Disease (PD) include physiotherapy, drugs, and invasive deep-brain stimulation (DBS), however, their efficacy tends to decrease as the disease progresses. Electrophysiological research investigating pharmacotherapy and DBS treatments led to a well-established understanding of neural circuit alterations underlying PD. Among these, event-related desynchronization (ERD) is a movement-related and frequency-band-specific increase in power that is typically elevated in PD patients and associated with motor symptoms. Our goal is to link knowledge about the pathophysiology of PD to markers of skilled human movement. We began by analysing the cqEEG dataset for the relationship between reaction time (RT) and ERD, and then started to build on the results by developing a non-invasive neurofeedback brain-computer interface (BCI) application which is intended to provide an alternative to existing treatments for PD patients. Here, we’re emphasizing that remote rehabilitation in patient’s homes allows for much more flexible and accessible therapy, which is why we’re working with a mobile EEG setup.

### cqEEG
We addressed whether neural features that are known to be dysregulated in PD predict the initiation time of a prepared finger sequence. Therefore, we analysed data from a study where 19 healthy participants trained to retrieve and produce a four-finger press sequence with two different timings (slow or fast) after a 1500 ms preparation period. We extracted the power spectra with Morlet wavelets from the electroencephalogram (EEG) and correlated them with behavioral markers via linear mixed modelling (LMM). Results of the LMMs, against our expectations, didn’t seem to provide evidence that alpha/mu (8-12 Hz) or beta power (13-20 Hz) are correlated with reaction time (RT) for correct sequence initiation (p>.05). Testing for ERD during movement preparation, a cluster-based permutation test revealed a significant deviation of beta power from baseline in the latency range from 200 to 600 ms for both (slow and fast sequence execution) conditions (p<.05). Based on these results, and to avoid limitations originating from the fixed preparation period employed in this task, we adapted the experiment and recorded the simpleRT dataset (see below).

![cqEEG](/img/cqEEG.jpg)

### openBCI
The simpleRT dataset was recorded with a custom openBCI EEG headset. We ordered their All-in-One Biosensing R&D Bundle (Medium / Short (0.5m cables)) and ThinkPulse™ Active Electrodes. The ThinkPulse™ electrodes are dry, active electrodes, which is great for quick setup times with good signal quality, but has drawbacks concerning [wearing comfort](https://github.com/geigermartin/neurofeedbackBCI/blob/main/openBCI/comfort.txt). We adapted the headset frame such that more channels could be placed over sensorimotor areas.  This decision was based on analyses of the cqEEG data, investigating where the strongest ERD occurs for each subject. Furthermore, clusters of electrodes, compared to only C3/4, seemed more robust for detecting relationships between ERD and RT. This informed our decision about how we wanted to assemble the new headset. Ultimately, we decided to record these 16 channels according to the 10-20 system: Fp1, Fp2, C5, C3, C1, C2, C4, C6, CP5, CP3, CP1, CP2, CP4, CP6, O1, O2. This was made possible by downloading the headset frame .stl files from OpenBCI, which are fortunately open source, and inserting additional channel locations that weren’t available yet via Blender (files are in this repo), before 3D printing the custom headset with a BambuLab X1C (at the UoB School of Engineering Makerspace). 
(PS: You have to change all channel gains to x1 if you’re using the ThinkPulse™ electrodes in combination with the OpenBCI GUI, otherwise all channels will be shown as railed with only flat data displayed as the raw signal.)

![oBCI](/img/openBCIheadset.jpg)

### Software
To stream, record, visualise, and interact with our data in real-time, we implemented the following software environment: The EEG and analog data are sent from the OpenBCI Cyton+Daisy amplifier to the USB dongle that is plugged into a laptop USB port. We read the data with the OpenBCI GUI. Here, we visualise it during setup and recording, and stream it to the LabStreamingLayer ([LSL](https://github.com/labstreaminglayer/pylsl)). We record all data streamed to LSL with the [Lab Recorder](https://github.com/labstreaminglayer/App-LabRecorder) and interface with VSCode. VSCode and Python are used to write the experiment, simulation, and neurofeedback scripts. Stimulus presentation is done via PsychoPy. Conda is used for package management.
Have a look at the [video documentation](https://youtu.be/Gwx8eSXYKEs) to see how the software works in practice. 

To develop the neurofeedback algorithm, we simulate data and stream it from VSCode to LSL, and with another script we can read the sim data in VSCode from LSL and process it in real-time exactly the same way as we would with real data. To provide haptic feedback in real-time we’re using an Arduino(v1) or ESP32(v2), which are programmed via the Arduino IDE. 

(PS: A note on streaming data to LSL. I first tried to use OpenBCI_LSL, which provides a command line interface to stream the data, but it’s very outdated and I couldn’t make it work properly even after porting it to PyQt5. Brainflow also didn’t work for me, but I didn’t spend too much time trying to figure it out. Ultimately, because the sampling rate of my analog data (required for trigger synchronization, see below) was too low with the official version of the OpenBCI GUI, I ended up using the latest beta (v6.0.0-beta.1), where everything works nicely.)

![softwareEnvironment](/img/softwareEnvironment.jpg)

### Trigger synchronization
My initial plan was to have software triggers via LSL and hardware triggers via a parallel port. However, the OpenBCI Cyton+Daisy amplifier is limited to three analog OR digital channels, but can’t stream both simultaneously. I considered building an optocoupler circuit to translate the digital to an analog signal but decided against it because of time constraints. We’re currently only recording EEG and therefore our major concern is display latency for stimulus presentation. To account for this we ended up using a photoresistor and software triggers. I soldered a simple [photoresistor circuit]( https://github.com/geigermartin/neurofeedbackBCI/blob/main/triggerSync/photoresistor.pdf) and connected it to the amplifier’s analog pins (PS: A nice trick is to use a transparent shrinking tube because it protects the circuit while also letting light through for detection by the photoresistor). This way, the EEG data and photoresistor data are recorded simultaneously by the same amplifier which enables us to correct the timing of our event markers like this: Whenever the go cue is displayed we also flick a little square on the monitor edge from black to white. The photoresistor is placed over this little square and detects this change in light intensity. In our offline analysis, we threshold the analog data to know the time when the stimulus was actually displayed. This information can then be used to shift the timestamps of our epoched data to account for the display latency. 

One caveat with the threshold of the photoresistor signal is that the Cyton+Daisy amplifier has a maximal sampling rate of 125 Hz if we’re recording 16 EEG channels (would be 250 Hz for 8 channels). Therefore, we get a sample every 8 ms. This means that the first sample that shows a value larger than our defined threshold can be anywhere between 0 to 8 ms after the signal actually passed the threshold. This leaves a remaining jitter of &plusmn; 4 ms of uncertainty. However, we can use interpolation to get even more accuracy in determining the threshold crossing of our analog signal and therefore in our latency correction. By interpolating the last sample before and the first sample after the signal crosses the threshold, we can determine the timepoint where this occurred with much more accuracy. This way we can get down to millisecond accuracy in aligning our EEG data and behavioural responses with the displayed stimuli which is crucial for analyses of event-related designs and RT paradigms. Here is a comparison of the raw photosensor data with the interpolated data that is aligned with high precision to an amplitude threshold of 700 a.u.:

![triggerSync](/img/triggerSync.jpg)

### simpleRT
We suspected that the 1500 ms preparation period in the cqEEG dataset allows participants to predict the go cue and as a result obscures the relationship between mu/beta power and RT. Therefore, we carried out a simple RT task, where 19 healthy participants trained to execute one four-finger press sequence with their non-dominant left hand after a variable preparation period (uniformly distributed along 9 equidistant values between 1 and 5 s (i.e. 1, 1.5, 2, …, 5 s)). They were instructed to carry out the sequence as fast and accurately as possible after the go cue. EEG data and RTs were corrected offline for delays in display latency as described above. We only got started analysing this data recently and will report on the results later.

### nfbBCI
We began to build on the results described above by developing a non-invasive EEG neurofeedback application that is intended to provide an alternative to existing treatments for PD and stroke patients. I started out with a **beta ERD simulation** to develop the real-time neurofeedback algorithm: 

![ERDsim](/img/ERDsim.jpg)

The 16 channel synthetic data, containing the beta ERD in one channel, is streamed to LSL, and with another script read from LSL for real-time processing. Here, it’s first added to a ring buffer and then processed in different ways. There are currently two approaches under development, we’ll see which works better in the future. The first algorithm is adapted from [He et al., 2020](https://doi.org/10.1523/JNEUROSCI.0208-20.2020) and utilizes a windowing approach, where the data is padded to avoid filter artefacts, then bandpass filtered (for low beta 13-20 Hz), and after removing the padding the median power for each window is calculated. This value is then compared to a threshold that is determined based on a baseline recording. If the correct neural state is achieved and held for a certain amount of time, haptic feedback is given through a vibration wristband:

![rtNFB](/img/rtNFB.jpg)

My concern with this approach is that it either introduces delays if only left-sided padding with real historical data is applied, or that the filter that allows to circumvent this delay is non-causal (filtfilt). When using reflective padding on the right side of the processing window, the signal alterations introduced by the reflectively padded data seems so be negligible, however, and this approach might be a valid option.

Approach two is based on a FIR filter. This removes the need for processing the data in these windows and is causal, while still allowing for very past processing with minimal delays by choosing the right amount of data and filter order. 

There is research about developing methods for exactly these purposes, e.g. [Smetanin et al., 2020](https://doi.org/10.1088/1741-2552/ab890f). It would be very interesting to implement their least-squares complex-valued filters (LSCF) for comparison, as well as to look for other approaches that are out there. 

Then, after analysing the simpleRT data offline and thereby identifying features that significantly predict behaviour, we can stream the simpleRT data into the neurofeedback algorithms to test them on and fine-tune them to real data.

This video shows the simulation approach. The upper plot displays two of the 16 synthetic data channels, containing beta ERD in the pink channel. Below that you see the triggers indicating when the ERD occurs, which is variable in length, interleaved by inter-trial-intervals (ITI) without ERD. These two are the data and marker streams that are streamed to LSL from the simulation script, and then picked up and processed by the neurofeedback script. The third plot shows bandpass filtered data (where reflective padding has been applied before and removed after filtering according to the schematic above). The fourth row shows median power values that are calculated for each of the processing windows, so it nicely tracks the evolution of beta power. The time window displayed in the video is 10s, the power values are updated every 125ms when they have been calculated for the current processing window (which are 250 ms and overlapping by half of their size). You will notice that only the pink but not the violet channels contains an ERD. And the fifth row shows the output of the real-time neurofeedback algorithm. A go-cue is generated when median low beta power of three adjacent processing windows is below the threshold. This equates to holding low beta below the threshold for 500 ms. You can also see that the feedback is specific to the ERD period, as it should be, and only very rarely triggered during an ITI. The low beta ERD embedded in the pink channel entails a 90% reduction in beta power and is therefore quite easy to pick up. This will be much more complicated with real data where such a big reduction will not be voluntarily possible straight away. So there will be challenges to adapt the algorithm to reliably pick up on initially much less voluntary reduction in beta power and to train participants to increase it over time. Well…here’s the output of my simulation (rows 1-2) and the windowing approach of the real-time neurofeedback algorithm (rows 3-5):

[![ERD_sim_video](/img/ERDsim_thumbnail.jpg)](https://youtu.be/-L9Hhc43kF0)

### Vibration wristband for haptic feedback
We want to provide haptic feedback, as opposed to the more common visual or auditory modalities. Therefore, I started out building a vibration wristband for haptic feedback. As soon as we wish to give feedback, e.g. if beta power is below a certain threshold for a defined time period, one or more small motors vibrate. Basically, when the algorithm above outputs a go cue (fifth plot), we send a command to the µC to vibrate (of course only one vibration per trial, not multiple as in the video).

**v1** consists of these components: Arduino Nano 33 IoT, N-Channel Mosfet 2N7000, Buck converter Y4183, Switch SS-12F1G3, 4x micro vibration motors (DC 3V, 12000 RPM, 10mm x 2.7mm), and a rechargeable 9V 1300 mAh battery. The Arduino connects to a Wifi hotspot on the laptop, through which we can send commands from our neurofeedback script in VS Code. The battery provides power for the motors and the Arduino. The voltage for the Arduino is downregulated with the buck converter. The switch turns everything on/off. And the Mosfet is necessary to control the vibration motors with the Arduino. All the components except the motors are placed inside a 3D printed case. The four motors can be adjusted freely around the wrist.

![hapticFeedback_v1](/img/hapticFeedback_v1.jpg)

**v2** is getting close to being finished. There were two major changes that I wanted to implement after v1. The first is the size – the whole thing can be significantly smaller. This is achieved by combining all of the components mentioned above into a single self-designed PCB. That’s the first time I designed a PCB, so I’m sure there’s room for improvement, but it should nevertheless be well suited for our application. And it was a fantastic learning experience (Thanks for your explanations, Tonderai!). The second improvement is based on the fact that neurofeedback is extremely time-sensitive. If someone reaches a brain state that we want to train them towards, then we have to deliver the feedback as fast as possible. If the data processing and transmission takes too long, then the subject’s brains will have moved on to another state by the time they receive feedback and learn unintended associations, which renders the application useless. So, in v2 I’m using a vibration motor that has a reaction time of only 12 ms. This is extremely fast comparable to everything else I could find. It furthermore has a much higher vibration strength, which is why I decided to place only one motor inside the case, instead of four around the wrist. By the time of writing this, I have finished the schematic and PCB layout and ordered all the components. What remains is to assemble them, and program it. Fingers crossed!

![hapticFeedback_v2](/img/hapticFeedback_v2.jpg)

### Posters & Presentations
If you’re interested in more detail about the data analyses and results, or the neurofeedback algorithm, please have a look at the [presentation](https://github.com/geigermartin/neurofeedbackBCI/blob/main/posters_presentations/presentations/240610_Geiger%26Kornysheva_NCode_expo.pptx) Katja and I gave at the [N-CODE Expo 2024](https://www.n-code.org/activity/n-code-expo-2024/), and the [poster](https://github.com/geigermartin/neurofeedbackBCI/blob/main/posters_presentations/posters/240617_cambridge.pdf) I presented at the [UK Sensorimotor Conference 2024 in Cambridge](https://x.com/UKSM2024). The information there is somewhat outdated already, but it should give you a better understanding of what some of the things I described above look like.

### Outlook
Where are we currently at and what can be done from here? We need to analyse the simpleRT EEG dataset to look for features that could be targeted with neurofeedback to improve movement in PD and stroke patients. Bursts may more promising than power because they carry more information and, in my understanding, an ERD might be the result of averaging over many trials with bursts. We can then stream the simpleRT EEG data into our neurofeedback real-time algorithm and fine-tune the most promising one(s) to reliably pick up on the features we identified for training. Then a neurofeedback study with healthy participants needs to be carried out to confirm the potential positive effects of neurofeedback on these neural features on the behavioral measure(s). Depending on the results, the paradigm could then potentially be adapted to and tested on patients. Simultaneously to all of that, the hardware can be developed to provide a comfortable EEG headset, containing only the relevant channels (potentially concealed within an accessoire like a hat), alongside an easy-to-use software to provide a suitable package for remote rehabilitation in patient’s homes.

Please feel free to get in contact if you have any questions about details of the analyses, setup, hardware, …, or anything else!!!

